{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Problem 1: Linear Regression Model (38 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a Linear Regression model using both Normal Equation Method and SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages to the jupyter notebook\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-05-15</td>\n",
       "      <td>2.437500</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.927083</td>\n",
       "      <td>1.958333</td>\n",
       "      <td>1.958333</td>\n",
       "      <td>72156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-05-16</td>\n",
       "      <td>1.968750</td>\n",
       "      <td>1.979167</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.729167</td>\n",
       "      <td>1.729167</td>\n",
       "      <td>14700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-05-19</td>\n",
       "      <td>1.760417</td>\n",
       "      <td>1.770833</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>6106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-05-20</td>\n",
       "      <td>1.729167</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>5467200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-05-21</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.645833</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>18853200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>1902.000000</td>\n",
       "      <td>1956.489990</td>\n",
       "      <td>1889.290039</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>6221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>1930.859985</td>\n",
       "      <td>1939.790039</td>\n",
       "      <td>1899.920044</td>\n",
       "      <td>1900.099976</td>\n",
       "      <td>1900.099976</td>\n",
       "      <td>5387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>1922.829956</td>\n",
       "      <td>1973.630005</td>\n",
       "      <td>1912.339966</td>\n",
       "      <td>1963.949951</td>\n",
       "      <td>1963.949951</td>\n",
       "      <td>6126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>1964.349976</td>\n",
       "      <td>1993.020020</td>\n",
       "      <td>1944.010010</td>\n",
       "      <td>1949.719971</td>\n",
       "      <td>1949.719971</td>\n",
       "      <td>5123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>1932.969971</td>\n",
       "      <td>1944.959961</td>\n",
       "      <td>1893.000000</td>\n",
       "      <td>1907.699951</td>\n",
       "      <td>1907.699951</td>\n",
       "      <td>4111100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5758 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date         Open         High          Low        Close  \\\n",
       "0     1997-05-15     2.437500     2.500000     1.927083     1.958333   \n",
       "1     1997-05-16     1.968750     1.979167     1.708333     1.729167   \n",
       "2     1997-05-19     1.760417     1.770833     1.625000     1.708333   \n",
       "3     1997-05-20     1.729167     1.750000     1.635417     1.635417   \n",
       "4     1997-05-21     1.635417     1.645833     1.375000     1.427083   \n",
       "...          ...          ...          ...          ...          ...   \n",
       "5753  2020-03-26  1902.000000  1956.489990  1889.290039  1955.489990   \n",
       "5754  2020-03-27  1930.859985  1939.790039  1899.920044  1900.099976   \n",
       "5755  2020-03-30  1922.829956  1973.630005  1912.339966  1963.949951   \n",
       "5756  2020-03-31  1964.349976  1993.020020  1944.010010  1949.719971   \n",
       "5757  2020-04-01  1932.969971  1944.959961  1893.000000  1907.699951   \n",
       "\n",
       "        Adj Close    Volume  \n",
       "0        1.958333  72156000  \n",
       "1        1.729167  14700000  \n",
       "2        1.708333   6106800  \n",
       "3        1.635417   5467200  \n",
       "4        1.427083  18853200  \n",
       "...           ...       ...  \n",
       "5753  1955.489990   6221300  \n",
       "5754  1900.099976   5387900  \n",
       "5755  1963.949951   6126100  \n",
       "5756  1949.719971   5123600  \n",
       "5757  1907.699951   4111100  \n",
       "\n",
       "[5758 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and load the csv data file\n",
    "\n",
    "filename = \"AMZN.csv\"\n",
    "\n",
    "data_file = read_csv(filename)\n",
    "\n",
    "# data\n",
    "\n",
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted Close Price df\n",
    "\n",
    "data_new = data_file[['Adj Close']]\n",
    "\n",
    "# converting dataset to numpy array\n",
    "\n",
    "values_new_df= data_new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.729167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.635417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.427083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>1955.489990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>1900.099976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>1963.949951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>1949.719971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>1907.699951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5758 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Adj Close\n",
       "0        1.958333\n",
       "1        1.729167\n",
       "2        1.708333\n",
       "3        1.635417\n",
       "4        1.427083\n",
       "...           ...\n",
       "5753  1955.489990\n",
       "5754  1900.099976\n",
       "5755  1963.949951\n",
       "5756  1949.719971\n",
       "5757  1907.699951\n",
       "\n",
       "[5758 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (a) Use the Python function (given at the end of the document) named **series to supervised()** that takes a univariate or multivariate time series and frames it as a supervised learning dataset (10 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "\"\"\"\n",
    "Frame a time series as a supervised learning dataset.\n",
    "Arguments:\n",
    "  data: Sequence of observations as a list or NumPy array.\n",
    "  n_in: Number of lag observations as input (X).\n",
    "  n_out: Number of observations as output (y).\n",
    "  dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "Returns:\n",
    "  Pandas DataFrame of series framed for supervised learning.\n",
    "\"\"\"\n",
    "\n",
    "def series_to_supervised(data_file, n_in=1, n_out=1, dropnan=True): \n",
    "    \n",
    "    n_vars = 1 if type(data_file) is list else data_file.shape[1] \n",
    "    df = DataFrame(data_file)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1): \n",
    "        cols.append(df.shift(i)) \n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1,... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)] \n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)] \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True) \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-10)</th>\n",
       "      <th>var1(t-9)</th>\n",
       "      <th>var1(t-8)</th>\n",
       "      <th>var1(t-7)</th>\n",
       "      <th>var1(t-6)</th>\n",
       "      <th>var1(t-5)</th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>var1(t-3)</th>\n",
       "      <th>var1(t-2)</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.958333</td>\n",
       "      <td>1.729167</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.729167</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "      <td>1.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "      <td>1.479167</td>\n",
       "      <td>1.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "      <td>1.479167</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>1.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>1676.609985</td>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1689.150024</td>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1689.150024</td>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>1689.150024</td>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "      <td>1963.949951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "      <td>1963.949951</td>\n",
       "      <td>1949.719971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "      <td>1963.949951</td>\n",
       "      <td>1949.719971</td>\n",
       "      <td>1907.699951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5748 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       var1(t-10)    var1(t-9)    var1(t-8)    var1(t-7)    var1(t-6)  \\\n",
       "10       1.958333     1.729167     1.708333     1.635417     1.427083   \n",
       "11       1.729167     1.708333     1.635417     1.427083     1.395833   \n",
       "12       1.708333     1.635417     1.427083     1.395833     1.500000   \n",
       "13       1.635417     1.427083     1.395833     1.500000     1.583333   \n",
       "14       1.427083     1.395833     1.500000     1.583333     1.531250   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "5753  1676.609985  1785.000000  1689.150024  1807.839966  1830.000000   \n",
       "5754  1785.000000  1689.150024  1807.839966  1830.000000  1880.930054   \n",
       "5755  1689.150024  1807.839966  1830.000000  1880.930054  1846.089966   \n",
       "5756  1807.839966  1830.000000  1880.930054  1846.089966  1902.829956   \n",
       "5757  1830.000000  1880.930054  1846.089966  1902.829956  1940.099976   \n",
       "\n",
       "        var1(t-5)    var1(t-4)    var1(t-3)    var1(t-2)    var1(t-1)  \\\n",
       "10       1.395833     1.500000     1.583333     1.531250     1.505208   \n",
       "11       1.500000     1.583333     1.531250     1.505208     1.500000   \n",
       "12       1.583333     1.531250     1.505208     1.500000     1.510417   \n",
       "13       1.531250     1.505208     1.500000     1.510417     1.479167   \n",
       "14       1.505208     1.500000     1.510417     1.479167     1.416667   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "5753  1880.930054  1846.089966  1902.829956  1940.099976  1885.839966   \n",
       "5754  1846.089966  1902.829956  1940.099976  1885.839966  1955.489990   \n",
       "5755  1902.829956  1940.099976  1885.839966  1955.489990  1900.099976   \n",
       "5756  1940.099976  1885.839966  1955.489990  1900.099976  1963.949951   \n",
       "5757  1885.839966  1955.489990  1900.099976  1963.949951  1949.719971   \n",
       "\n",
       "          var1(t)  \n",
       "10       1.500000  \n",
       "11       1.510417  \n",
       "12       1.479167  \n",
       "13       1.416667  \n",
       "14       1.541667  \n",
       "...           ...  \n",
       "5753  1955.489990  \n",
       "5754  1900.099976  \n",
       "5755  1963.949951  \n",
       "5756  1949.719971  \n",
       "5757  1907.699951  \n",
       "\n",
       "[5748 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#checking the ouput by passing the values_new_df into series to supervised parameter\n",
    "data = series_to_supervised(values_new_df, 10)\n",
    "\n",
    "# transformed dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5748, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trans_data data frame shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Use **MinMaxScaler** to scale your data. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5748, 11)\n",
      "[[2.59357128e-04 1.53693076e-04 1.44087293e-04 ... 5.28325334e-05\n",
      "  4.08251620e-05 3.84236657e-05]\n",
      " [1.53693076e-04 1.44087293e-04 1.10466888e-04 ... 4.08251620e-05\n",
      "  3.84236657e-05 4.32266033e-05]\n",
      " [1.44087293e-04 1.10466888e-04 1.44087293e-05 ... 3.84236657e-05\n",
      "  4.32266033e-05 2.88177355e-05]\n",
      " ...\n",
      " [7.78188587e-01 8.32914067e-01 8.43131601e-01 ... 9.00991491e-01\n",
      "  8.75452055e-01 9.04892242e-01]\n",
      " [8.32914067e-01 8.43131601e-01 8.66614396e-01 ... 8.75452055e-01\n",
      "  9.04892242e-01 8.98331029e-01]\n",
      " [8.43131601e-01 8.66614396e-01 8.50550352e-01 ... 9.04892242e-01\n",
      "  8.98331029e-01 8.78956280e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the data set into a numpy array \n",
    "array = data.values\n",
    "# Define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "# Transformed the data\n",
    "file_new = scaler.fit_transform(array)\n",
    "# data shape\n",
    "print(file_new.shape)\n",
    "# Visualize the scaled data\n",
    "print(file_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Use the Normal Equation Method to find the linear regression coefficients (w). To perform this you may want to take the following steps first: Split your data to X and Y by taking the columns var1(t-10),...,var(t-1) as your 10 features in X, and take the last column var1(t) as your target (Y). Expand your matrix X with a bias vector of ones as the first column (to accomplish this, you may want to use the numpy operations **np.ones , np.reshape and np.append** ).\n",
    "Use the **train test split** with â€˜random state=1â€™ to split your data to 70% training, and 30% test data. Solve the Normal Equation Method in (2) to find the coefficients w. (10 point).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "X = file_new[:,0:10]\n",
    "y = file_new[:,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand the matirx X with a bias vector of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5748,)\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a  vector with 5748 (total of observations) values\n",
    "vector = np.ones(5748)\n",
    "# Vector shape\n",
    "print(vector.shape)\n",
    "# Print vector to visualize its shape\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5748, 1)\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the vector \n",
    "\n",
    "vector=np.reshape(vector, (5748,1))\n",
    "\n",
    "# New Shape\n",
    "print(vector.shape)\n",
    "\n",
    "# Print new vector \n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5748, 11)\n"
     ]
    }
   ],
   "source": [
    "# Add vector as a first column to the matriz X\n",
    "\n",
    "X = np.append(vector, X, axis=1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data using train and test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal equation implementation in python using numpy for linear regression\n",
    "\n",
    "def LinearnormalEquation(X, y):\n",
    "   \n",
    "    X_transpose = np.transpose(X)\n",
    "\n",
    "    # Calculating theta\n",
    "    theta = np.linalg.inv(X_transpose.dot(X))\n",
    "    theta = theta.dot(X_transpose)\n",
    "    theta = theta.dot(y)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.26111054e-04 -4.83348220e-02  2.24558809e-01 -1.52228741e-01\n",
      "  3.28901068e-02 -1.08811996e-01  7.17528590e-02 -2.60760209e-02\n",
      " -1.48358090e-03  8.01298345e-02  9.28047368e-01]\n"
     ]
    }
   ],
   "source": [
    "# Normal equation for linear regression\n",
    "theta = np.dot(np.linalg.inv(np.dot(X_train.T,X_train)),np.dot(X_train.T,y_train))\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Make a prediction on your test set using the linear regression function f(x) = wTx, and use both the mean square error and coefficient of determination R2 to measure the performance of your prediction model. For this use fucntions **mean_squared_error** and **r2** score from sklearn library. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to obtain prediction on y using coefficients(theta)\n",
    "\n",
    "def predict(X, theta):\n",
    "    return X.dot(theta)\n",
    "    \n",
    "\n",
    "# Calculate the predictions \n",
    "\n",
    "preds = predict(X_test, theta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for the model is: 2.4148595608753934e-05\n"
     ]
    }
   ],
   "source": [
    "#predicting using mean square error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse=mean_squared_error(y_test,preds )\n",
    "print('The MSE for the model is:' , mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is: 0.9995593086029828\n"
     ]
    }
   ],
   "source": [
    "#predicting using R2\n",
    "from sklearn.metrics import r2_score\n",
    "r2=r2_score(y_test, preds)\n",
    "print(\"R2 is:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(e) Next, find the coefficients w using gradient descent algorithm and monitor how your error changes in each epoch; You can create a function **coefficients_sgd** similar to what we did in our Lab Session 7. Note that you may have to make some minor changes to this part of the code (**coefficients_sgd** for linear regression, in lab session 7), due to the additional bias term 1 in your matrix X. For this part, use learning rate 0.01, and number of epochs (iterations) 200. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficients_sgd(X_train, Y_train, l_rate, n_epoch): #l-rate is learning rate\n",
    "  #initializing all coefficients to zero\n",
    "  coef = [0.0 for i in range(len(X_train[0]))]\n",
    "  for epoch in range(n_epoch + 1):\n",
    "    sum_error = 0 # loss\n",
    "    for i in range(X_train.shape[0]):\n",
    "      # calculating the prediction using current coeeficients\n",
    "      yhat = predict(X_train[i,:], coef)\n",
    "      # calculating error\n",
    "      error = yhat - Y_train[i] #yhat is prediction, Y_train is ground truth,\n",
    "      sum_error += error**2 # error square, because loss cannot be negative, or we want to error to be positive.\n",
    "      #stochastic gradient descent\n",
    "      coef[0] = coef[0] - l_rate * error\n",
    "      for j in range(len(coef)-1):\n",
    "        coef[j + 1] = coef[j + 1] - l_rate * error * X_train[i,j]\n",
    "    \n",
    "    print( ' >epoch=%d, lrate=%.3f, error=%.3f ' % (epoch, l_rate, sum_error))\n",
    "  #returning the list of coefficients  \n",
    "  return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >epoch=0, lrate=0.010, error=5.718 \n",
      " >epoch=1, lrate=0.010, error=0.423 \n",
      " >epoch=2, lrate=0.010, error=0.420 \n",
      " >epoch=3, lrate=0.010, error=0.416 \n",
      " >epoch=4, lrate=0.010, error=0.413 \n",
      " >epoch=5, lrate=0.010, error=0.410 \n",
      " >epoch=6, lrate=0.010, error=0.407 \n",
      " >epoch=7, lrate=0.010, error=0.404 \n",
      " >epoch=8, lrate=0.010, error=0.401 \n",
      " >epoch=9, lrate=0.010, error=0.399 \n",
      " >epoch=10, lrate=0.010, error=0.396 \n",
      " >epoch=11, lrate=0.010, error=0.393 \n",
      " >epoch=12, lrate=0.010, error=0.390 \n",
      " >epoch=13, lrate=0.010, error=0.387 \n",
      " >epoch=14, lrate=0.010, error=0.385 \n",
      " >epoch=15, lrate=0.010, error=0.382 \n",
      " >epoch=16, lrate=0.010, error=0.380 \n",
      " >epoch=17, lrate=0.010, error=0.377 \n",
      " >epoch=18, lrate=0.010, error=0.374 \n",
      " >epoch=19, lrate=0.010, error=0.372 \n",
      " >epoch=20, lrate=0.010, error=0.369 \n",
      " >epoch=21, lrate=0.010, error=0.367 \n",
      " >epoch=22, lrate=0.010, error=0.365 \n",
      " >epoch=23, lrate=0.010, error=0.362 \n",
      " >epoch=24, lrate=0.010, error=0.360 \n",
      " >epoch=25, lrate=0.010, error=0.358 \n",
      " >epoch=26, lrate=0.010, error=0.355 \n",
      " >epoch=27, lrate=0.010, error=0.353 \n",
      " >epoch=28, lrate=0.010, error=0.351 \n",
      " >epoch=29, lrate=0.010, error=0.349 \n",
      " >epoch=30, lrate=0.010, error=0.347 \n",
      " >epoch=31, lrate=0.010, error=0.344 \n",
      " >epoch=32, lrate=0.010, error=0.342 \n",
      " >epoch=33, lrate=0.010, error=0.340 \n",
      " >epoch=34, lrate=0.010, error=0.338 \n",
      " >epoch=35, lrate=0.010, error=0.336 \n",
      " >epoch=36, lrate=0.010, error=0.334 \n",
      " >epoch=37, lrate=0.010, error=0.332 \n",
      " >epoch=38, lrate=0.010, error=0.330 \n",
      " >epoch=39, lrate=0.010, error=0.329 \n",
      " >epoch=40, lrate=0.010, error=0.327 \n",
      " >epoch=41, lrate=0.010, error=0.325 \n",
      " >epoch=42, lrate=0.010, error=0.323 \n",
      " >epoch=43, lrate=0.010, error=0.321 \n",
      " >epoch=44, lrate=0.010, error=0.319 \n",
      " >epoch=45, lrate=0.010, error=0.318 \n",
      " >epoch=46, lrate=0.010, error=0.316 \n",
      " >epoch=47, lrate=0.010, error=0.314 \n",
      " >epoch=48, lrate=0.010, error=0.313 \n",
      " >epoch=49, lrate=0.010, error=0.311 \n",
      " >epoch=50, lrate=0.010, error=0.309 \n",
      " >epoch=51, lrate=0.010, error=0.308 \n",
      " >epoch=52, lrate=0.010, error=0.306 \n",
      " >epoch=53, lrate=0.010, error=0.304 \n",
      " >epoch=54, lrate=0.010, error=0.303 \n",
      " >epoch=55, lrate=0.010, error=0.301 \n",
      " >epoch=56, lrate=0.010, error=0.300 \n",
      " >epoch=57, lrate=0.010, error=0.298 \n",
      " >epoch=58, lrate=0.010, error=0.297 \n",
      " >epoch=59, lrate=0.010, error=0.295 \n",
      " >epoch=60, lrate=0.010, error=0.294 \n",
      " >epoch=61, lrate=0.010, error=0.293 \n",
      " >epoch=62, lrate=0.010, error=0.291 \n",
      " >epoch=63, lrate=0.010, error=0.290 \n",
      " >epoch=64, lrate=0.010, error=0.288 \n",
      " >epoch=65, lrate=0.010, error=0.287 \n",
      " >epoch=66, lrate=0.010, error=0.286 \n",
      " >epoch=67, lrate=0.010, error=0.284 \n",
      " >epoch=68, lrate=0.010, error=0.283 \n",
      " >epoch=69, lrate=0.010, error=0.282 \n",
      " >epoch=70, lrate=0.010, error=0.281 \n",
      " >epoch=71, lrate=0.010, error=0.279 \n",
      " >epoch=72, lrate=0.010, error=0.278 \n",
      " >epoch=73, lrate=0.010, error=0.277 \n",
      " >epoch=74, lrate=0.010, error=0.276 \n",
      " >epoch=75, lrate=0.010, error=0.274 \n",
      " >epoch=76, lrate=0.010, error=0.273 \n",
      " >epoch=77, lrate=0.010, error=0.272 \n",
      " >epoch=78, lrate=0.010, error=0.271 \n",
      " >epoch=79, lrate=0.010, error=0.270 \n",
      " >epoch=80, lrate=0.010, error=0.269 \n",
      " >epoch=81, lrate=0.010, error=0.268 \n",
      " >epoch=82, lrate=0.010, error=0.267 \n",
      " >epoch=83, lrate=0.010, error=0.266 \n",
      " >epoch=84, lrate=0.010, error=0.264 \n",
      " >epoch=85, lrate=0.010, error=0.263 \n",
      " >epoch=86, lrate=0.010, error=0.262 \n",
      " >epoch=87, lrate=0.010, error=0.261 \n",
      " >epoch=88, lrate=0.010, error=0.260 \n",
      " >epoch=89, lrate=0.010, error=0.259 \n",
      " >epoch=90, lrate=0.010, error=0.258 \n",
      " >epoch=91, lrate=0.010, error=0.257 \n",
      " >epoch=92, lrate=0.010, error=0.256 \n",
      " >epoch=93, lrate=0.010, error=0.256 \n",
      " >epoch=94, lrate=0.010, error=0.255 \n",
      " >epoch=95, lrate=0.010, error=0.254 \n",
      " >epoch=96, lrate=0.010, error=0.253 \n",
      " >epoch=97, lrate=0.010, error=0.252 \n",
      " >epoch=98, lrate=0.010, error=0.251 \n",
      " >epoch=99, lrate=0.010, error=0.250 \n",
      " >epoch=100, lrate=0.010, error=0.249 \n",
      " >epoch=101, lrate=0.010, error=0.248 \n",
      " >epoch=102, lrate=0.010, error=0.248 \n",
      " >epoch=103, lrate=0.010, error=0.247 \n",
      " >epoch=104, lrate=0.010, error=0.246 \n",
      " >epoch=105, lrate=0.010, error=0.245 \n",
      " >epoch=106, lrate=0.010, error=0.244 \n",
      " >epoch=107, lrate=0.010, error=0.243 \n",
      " >epoch=108, lrate=0.010, error=0.243 \n",
      " >epoch=109, lrate=0.010, error=0.242 \n",
      " >epoch=110, lrate=0.010, error=0.241 \n",
      " >epoch=111, lrate=0.010, error=0.240 \n",
      " >epoch=112, lrate=0.010, error=0.240 \n",
      " >epoch=113, lrate=0.010, error=0.239 \n",
      " >epoch=114, lrate=0.010, error=0.238 \n",
      " >epoch=115, lrate=0.010, error=0.237 \n",
      " >epoch=116, lrate=0.010, error=0.237 \n",
      " >epoch=117, lrate=0.010, error=0.236 \n",
      " >epoch=118, lrate=0.010, error=0.235 \n",
      " >epoch=119, lrate=0.010, error=0.235 \n",
      " >epoch=120, lrate=0.010, error=0.234 \n",
      " >epoch=121, lrate=0.010, error=0.233 \n",
      " >epoch=122, lrate=0.010, error=0.233 \n",
      " >epoch=123, lrate=0.010, error=0.232 \n",
      " >epoch=124, lrate=0.010, error=0.231 \n",
      " >epoch=125, lrate=0.010, error=0.231 \n",
      " >epoch=126, lrate=0.010, error=0.230 \n",
      " >epoch=127, lrate=0.010, error=0.229 \n",
      " >epoch=128, lrate=0.010, error=0.229 \n",
      " >epoch=129, lrate=0.010, error=0.228 \n",
      " >epoch=130, lrate=0.010, error=0.228 \n",
      " >epoch=131, lrate=0.010, error=0.227 \n",
      " >epoch=132, lrate=0.010, error=0.226 \n",
      " >epoch=133, lrate=0.010, error=0.226 \n",
      " >epoch=134, lrate=0.010, error=0.225 \n",
      " >epoch=135, lrate=0.010, error=0.225 \n",
      " >epoch=136, lrate=0.010, error=0.224 \n",
      " >epoch=137, lrate=0.010, error=0.224 \n",
      " >epoch=138, lrate=0.010, error=0.223 \n",
      " >epoch=139, lrate=0.010, error=0.222 \n",
      " >epoch=140, lrate=0.010, error=0.222 \n",
      " >epoch=141, lrate=0.010, error=0.221 \n",
      " >epoch=142, lrate=0.010, error=0.221 \n",
      " >epoch=143, lrate=0.010, error=0.220 \n",
      " >epoch=144, lrate=0.010, error=0.220 \n",
      " >epoch=145, lrate=0.010, error=0.219 \n",
      " >epoch=146, lrate=0.010, error=0.219 \n",
      " >epoch=147, lrate=0.010, error=0.218 \n",
      " >epoch=148, lrate=0.010, error=0.218 \n",
      " >epoch=149, lrate=0.010, error=0.217 \n",
      " >epoch=150, lrate=0.010, error=0.217 \n",
      " >epoch=151, lrate=0.010, error=0.216 \n",
      " >epoch=152, lrate=0.010, error=0.216 \n",
      " >epoch=153, lrate=0.010, error=0.215 \n",
      " >epoch=154, lrate=0.010, error=0.215 \n",
      " >epoch=155, lrate=0.010, error=0.214 \n",
      " >epoch=156, lrate=0.010, error=0.214 \n",
      " >epoch=157, lrate=0.010, error=0.214 \n",
      " >epoch=158, lrate=0.010, error=0.213 \n",
      " >epoch=159, lrate=0.010, error=0.213 \n",
      " >epoch=160, lrate=0.010, error=0.212 \n",
      " >epoch=161, lrate=0.010, error=0.212 \n",
      " >epoch=162, lrate=0.010, error=0.211 \n",
      " >epoch=163, lrate=0.010, error=0.211 \n",
      " >epoch=164, lrate=0.010, error=0.211 \n",
      " >epoch=165, lrate=0.010, error=0.210 \n",
      " >epoch=166, lrate=0.010, error=0.210 \n",
      " >epoch=167, lrate=0.010, error=0.209 \n",
      " >epoch=168, lrate=0.010, error=0.209 \n",
      " >epoch=169, lrate=0.010, error=0.209 \n",
      " >epoch=170, lrate=0.010, error=0.208 \n",
      " >epoch=171, lrate=0.010, error=0.208 \n",
      " >epoch=172, lrate=0.010, error=0.207 \n",
      " >epoch=173, lrate=0.010, error=0.207 \n",
      " >epoch=174, lrate=0.010, error=0.207 \n",
      " >epoch=175, lrate=0.010, error=0.206 \n",
      " >epoch=176, lrate=0.010, error=0.206 \n",
      " >epoch=177, lrate=0.010, error=0.206 \n",
      " >epoch=178, lrate=0.010, error=0.205 \n",
      " >epoch=179, lrate=0.010, error=0.205 \n",
      " >epoch=180, lrate=0.010, error=0.204 \n",
      " >epoch=181, lrate=0.010, error=0.204 \n",
      " >epoch=182, lrate=0.010, error=0.204 \n",
      " >epoch=183, lrate=0.010, error=0.203 \n",
      " >epoch=184, lrate=0.010, error=0.203 \n",
      " >epoch=185, lrate=0.010, error=0.203 \n",
      " >epoch=186, lrate=0.010, error=0.202 \n",
      " >epoch=187, lrate=0.010, error=0.202 \n",
      " >epoch=188, lrate=0.010, error=0.202 \n",
      " >epoch=189, lrate=0.010, error=0.201 \n",
      " >epoch=190, lrate=0.010, error=0.201 \n",
      " >epoch=191, lrate=0.010, error=0.201 \n",
      " >epoch=192, lrate=0.010, error=0.201 \n",
      " >epoch=193, lrate=0.010, error=0.200 \n",
      " >epoch=194, lrate=0.010, error=0.200 \n",
      " >epoch=195, lrate=0.010, error=0.200 \n",
      " >epoch=196, lrate=0.010, error=0.199 \n",
      " >epoch=197, lrate=0.010, error=0.199 \n",
      " >epoch=198, lrate=0.010, error=0.199 \n",
      " >epoch=199, lrate=0.010, error=0.198 \n",
      " >epoch=200, lrate=0.010, error=0.198 \n"
     ]
    }
   ],
   "source": [
    "# set the learning rate and the # of epocs\n",
    "l_rate = 0.01\n",
    "n_epoch = 200\n",
    "\n",
    "# Calculate the coefficients using the \"coefficients_sgd\" function\n",
    "coef_sgd = coefficients_sgd(X_train, y_train, l_rate, n_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Make a prediction using the coefficients you found from SGD algorithm in previous step **(Y prediction sgd = X test.dot(coef sgd))**; Use both the mean square error and coeffi- cient of determination R2 to measure the performance of your predictions; compare the results with your prediction performance in part d where you used the coefficients found from Normal Equation Method. Which method gives you better results? (6 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predictions \n",
    "\n",
    "preds_sgd = predict(X_test, coef_sgd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for the model is: 3.899286562084085e-05\n"
     ]
    }
   ],
   "source": [
    "#predicting using mean square error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse=mean_squared_error(y_test,preds_sgd )\n",
    "print('The MSE for the model is:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is: 0.9992884132600274\n"
     ]
    }
   ],
   "source": [
    "#predicting using R2\n",
    "from sklearn.metrics import r2_score\n",
    "r2=r2_score(y_test, preds_sgd)\n",
    "print(\"R2 is:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After predicting the MSE and R2 for two models i.e Linear Regression with normal equation and linear regression using gradient descent algorithm better predictions have been Occurred for linear regression using normal equation which is the better model as it got 2.14 as MSE(the lower the mse the better model) and 0.99959(good value) for R2 where as gradient descent equation outputs are 3.899 for MSE and 0.999288 for R2 .I would prefer normal equation model as it metrics are more relatable to select as a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Problem 2: Create a Perceptron model with an optimal value of hyperparameter Î± (learning rate of SGD) (18 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and load the csv data file\n",
    "filename = \"sonar.all-data.csv\" \n",
    "\n",
    "data_csv = read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Split your data into train and test portions with â€˜test size = 0.3â€™ and â€™random state = 3â€™ . Define your learning model to be Perceptron. Use **RepeatedStratifiedKFold** with **â€˜n splits=10â€™, â€˜n repeats=5â€™, and â€˜random state=1â€™ as your model evaluation method. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the dataset to a numpy array\n",
    "\n",
    "array = data_csv.values\n",
    "\n",
    "# separate array into input and output components\n",
    "\n",
    "X = array[:,:-1] \n",
    "Y = array[:,-1]\n",
    "\n",
    "# Split the data into train and test portions\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.664 - Std: 0.119\n",
      "Accuracy: 0.651\n",
      "F1-Score: 0.500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define model\n",
    "model = Perceptron()\n",
    "# define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "results = cross_val_score(model, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print('Mean Accuracy: %.3f - Std: %.3f' % (results.mean(), results.std()))\n",
    "print('Accuracy: %.3f' % accuracy_score(pred, y_test))\n",
    "print('F1-Score: %.3f' % f1_score(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Use **GridSearchCV** to perform a gird search on the parameter of Perceptron algorithm (learn- ing rate Î± in SGD), consider values for Î± as [0.0001, 0.001, 0.01, 0.1]. For your GridSearch, use data only from your training sets (X train, Y train). (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define Grid \n",
    "\n",
    "grid = dict()\n",
    "grid['alpha'] = [0.0001, 0.001, 0.01, 0.1] # Regularization term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search\n",
    "\n",
    "search= GridSearchCV(model, grid, cv=cv, n_jobs=-1)\n",
    "\n",
    "# Perform the search \n",
    "\n",
    "results_grid = search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Report the best score and the best value of the parameter in your search. (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.664\n",
      "The best value of the parameter alpha is : {'alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "# summarize\n",
    "print('Mean Accuracy: %.3f' % results_grid.best_score_)\n",
    "print('The best value of the parameter alpha is : %s' % results_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Create a Perceptron model which takes as an argument the best value of parameter you found in the previous step, and use this model to make predictions on your test set; Report the accuracy. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy and std : 0.664 (0.119)\n",
      "Accuracy score of predicted: 0.651\n",
      "F1-Score: 0.500\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model_new=Perceptron(alpha=0.0001)\n",
    "# define model evaluation method\n",
    "cv_new = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv_new, n_jobs=-1)\n",
    "# summarize result\n",
    "print('Mean Accuracy and std : %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "model_new.fit(X_train, y_train)\n",
    "pred = model_new.predict(X_test)\n",
    "print('Accuracy score of predicted: %.3f' % accuracy_score(pred, y_test))\n",
    "print('F1-Score: %.3f' % f1_score(pred, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Problem 3: Create a KNN model with an optimal value of hyperparameter K (the number of nearest neighbors) (18 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages to the Jupyter notebook # Create a KNN model with the best parameter K import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Split the data into train and test sets with **â€˜test size = 0.3â€™, and â€˜random state = 5â€™**. Create a KNN model with parameter â€˜n neighborâ€™ varying from 1 to 30 (see the code from Lab Session 6). (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and load the csv data file\n",
    "filename = \"sonar.all-data.csv\" \n",
    "dataframe = read_csv(filename)\n",
    "\n",
    "#converting the dataset to a numpy array\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,:-1] \n",
    "Y = array[:,-1]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a model, evaluate abd predict the score \n",
    "scores = {}\n",
    "for k in range(1,31):\n",
    "  knn = KNeighborsClassifier(n_neighbors=k)\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  scores[k] = accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6984126984126984"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.7777777777777778,\n",
       " 2: 0.7142857142857143,\n",
       " 3: 0.7301587301587301,\n",
       " 4: 0.7142857142857143,\n",
       " 5: 0.746031746031746,\n",
       " 6: 0.746031746031746,\n",
       " 7: 0.6507936507936508,\n",
       " 8: 0.6349206349206349,\n",
       " 9: 0.6666666666666666,\n",
       " 10: 0.6666666666666666,\n",
       " 11: 0.6825396825396826,\n",
       " 12: 0.6507936507936508,\n",
       " 13: 0.6666666666666666,\n",
       " 14: 0.6507936507936508,\n",
       " 15: 0.6825396825396826,\n",
       " 16: 0.6666666666666666,\n",
       " 17: 0.6507936507936508,\n",
       " 18: 0.6666666666666666,\n",
       " 19: 0.6507936507936508,\n",
       " 20: 0.7142857142857143,\n",
       " 21: 0.6825396825396826,\n",
       " 22: 0.6825396825396826,\n",
       " 23: 0.6984126984126984,\n",
       " 24: 0.6825396825396826,\n",
       " 25: 0.6825396825396826,\n",
       " 26: 0.6825396825396826,\n",
       " 27: 0.6666666666666666,\n",
       " 28: 0.6984126984126984,\n",
       " 29: 0.7142857142857143,\n",
       " 30: 0.6984126984126984}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Plot the accuracy of the KNN model in terms of the number of nearest neighbor k varying from 1 to 30. Choose and report the best value for k. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(list(scores.keys()),list(scores.values()), c='b')\n",
    "plt.title('K-nearest', fontsize=16)\n",
    "plt.xlabel('K value of neighbors', fontsize=12)\n",
    "plt.ylabel('Accuracy value ', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Create a new KNN model with the best values of nearest neighbors that you found in previous step, and perform prediction on your test set. Report the accuracy of the model. (5 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best kneighbor is 1 from the above problem so selecting that to do the predictions in problem.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.708 (0.149)\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "knn= KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Fit the mode\n",
    "\n",
    "# Define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "\n",
    "# Evaluate model\n",
    "results_knn = cross_val_score(knn, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (results_knn.mean(), results_knn.std()))\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "scores = accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
